{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449df7a9",
   "metadata": {},
   "source": [
    "# Alseny\n",
    "### Data Scient & Machine Learning Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17664a34",
   "metadata": {},
   "source": [
    "# A- Packages nécessaires pour le projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6dba6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import calendar\n",
    "from sklearn.model_selection import train_test_split\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import KBinsDiscretizer  # Pour la discrétisation optimale\n",
    "import matplotlib.colors as mcolors\n",
    "import mapclassify as mc  # Nécessaire pour Jenks Natural Breaks\n",
    "from scipy.stats import iqr\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d76289",
   "metadata": {},
   "source": [
    "# B- Les fonctions utiles pour le projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5e1b9",
   "metadata": {},
   "source": [
    "### B.1- Chargement dess données et exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e649dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_save_data(file_path, sep=None, export=False, export_path=None,\n",
    "                       export_format=None, sample=None, random_state=None,\n",
    "                       chunksize=None, index_col=None):\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    chunk_supported = ext in ['.csv', '.txt']\n",
    "\n",
    "    def detect_separator(path):\n",
    "        for s in [',', ';', '\\t', '|']:\n",
    "            try:\n",
    "                if len(pd.read_csv(path, sep=s, nrows=5).columns) > 1:\n",
    "                    return s\n",
    "            except pd.errors.ParserError:\n",
    "                continue\n",
    "        raise ValueError(\"Séparateur non détecté. Spécifiez 'sep'.\")\n",
    "\n",
    "    def process_chunk(chunk):\n",
    "        return chunk  # Personnalisez si besoin\n",
    "\n",
    "    if chunksize and not chunk_supported:\n",
    "        raise ValueError(f\"Le format {ext} ne supporte pas 'chunksize'.\")\n",
    "\n",
    "    # ====================== Lecture par morceaux ======================\n",
    "    if chunksize:\n",
    "        sep = sep or detect_separator(file_path)\n",
    "        reader = pd.read_csv(file_path, sep=sep, chunksize=chunksize)\n",
    "\n",
    "        if export:\n",
    "            if not export_path:\n",
    "                raise ValueError(\"Spécifiez 'export_path' pour l'export.\")\n",
    "            export_ext = '.' + export_format.lower() if export_format else os.path.splitext(export_path)[1].lower()\n",
    "            if export_ext not in ['.csv', '.txt']:\n",
    "                raise ValueError(f\"Export par chunks non supporté pour {export_ext}\")\n",
    "            with open(export_path, 'w', encoding='utf-8', newline='') as f:\n",
    "                for i, chunk in enumerate(reader):\n",
    "                    chunk = process_chunk(chunk)\n",
    "                    if sample:\n",
    "                        chunk = chunk.sample(n=sample if isinstance(sample, int)\n",
    "                                             else None,\n",
    "                                             frac=sample if isinstance(sample, float)\n",
    "                                             else None,\n",
    "                                             random_state=random_state)\n",
    "                    chunk.to_csv(f, sep=sep, index=False, header=(i == 0), mode='a')\n",
    "        else:\n",
    "            for chunk in reader:\n",
    "                process_chunk(chunk)  # À adapter\n",
    "        return None\n",
    "\n",
    "    # ====================== Lecture complète ======================\n",
    "    if sep is None and ext in ['.csv', '.txt']:\n",
    "        sep = detect_separator(file_path)\n",
    "\n",
    "    read_funcs = {\n",
    "        '.csv': lambda: pd.read_csv(file_path, sep=sep, index_col=index_col),\n",
    "        '.txt': lambda: pd.read_csv(file_path, sep=sep, index_col=index_col),\n",
    "        '.json': lambda: pd.read_json(file_path),\n",
    "        '.xlsx': lambda: pd.read_excel(file_path),\n",
    "        '.pickle': lambda: pd.read_pickle(file_path),\n",
    "        '.parquet': lambda: pd.read_parquet(file_path)\n",
    "    }\n",
    "\n",
    "    if ext not in read_funcs:\n",
    "        raise ValueError(f\"Format non pris en charge : {ext}\")\n",
    "\n",
    "    df = read_funcs[ext]()\n",
    "\n",
    "    if sample:\n",
    "        df = df.sample(n=sample if isinstance(sample, int)\n",
    "                       else None,\n",
    "                       frac=sample if isinstance(sample, float)\n",
    "                       else None,\n",
    "                       random_state=random_state)\n",
    "\n",
    "    # ====================== Export ======================\n",
    "    if export:\n",
    "        export_ext = '.' + export_format.lower() if export_format else os.path.splitext(export_path)[1].lower()\n",
    "        if export_ext not in ['.csv', '.txt', '.json', '.xlsx', '.pickle', '.parquet']:\n",
    "            raise ValueError(f\"Export non supporté pour : {export_ext}\")\n",
    "        if export_ext in ['.csv', '.txt']:\n",
    "            df.to_csv(export_path, sep=sep or ',', index=False)\n",
    "        elif export_ext == '.json':\n",
    "            df.to_json(export_path, orient='records', lines=True)\n",
    "        elif export_ext == '.xlsx':\n",
    "            df.to_excel(export_path, index=False)\n",
    "        elif export_ext == '.pickle':\n",
    "            df.to_pickle(export_path)\n",
    "        elif export_ext == '.parquet':\n",
    "            df.to_parquet(export_path, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab462a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction d'identification de la nature des variables\n",
    "\n",
    "def compter_variables(dataset):\n",
    "    \"\"\"Affiche et compte les variables quantitatives et qualitatives d'un dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (pandas.DataFrame): Le DataFrame contenant les variables à analyser.\n",
    "\n",
    "    \"\"\"\n",
    "    # Sélection des colonnes quantitatives (numériques) et qualitatives (non numériques)\n",
    "    quantitatives = dataset.select_dtypes(include=['number']).columns\n",
    "    qualitatives = dataset.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "    # Affichage des colonnes quantitatives et de leur nombre\n",
    "    print(\"Les variables quantitatives:\\n\", quantitatives)\n",
    "    print(f\"Nombre de variables quantitatives : {len(quantitatives)}\\n\")\n",
    "\n",
    "    # Affichage des colonnes qualitatives et de leur nombre\n",
    "    print(\"Les variables qualitatives:\\n\", qualitatives)\n",
    "    print(f\"Nombre de variables qualitatives : {len(qualitatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369b54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des lignes dupliquées dans le DataFrame\n",
    "\n",
    "def duplicated_values_summary(df):\n",
    "    # Calcul de la somme totale des lignes dupliquées\n",
    "    total_duplicated_rows = df.duplicated().sum()\n",
    "\n",
    "    # Identification des colonnes contenant des valeurs dupliquées\n",
    "    duplicated_columns = [col for col in df.columns if df[col].duplicated().any()]\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Total de lignes dupliquées dans le DataFrame :\", total_duplicated_rows)\n",
    "    print(\"Colonnes contenant des valeurs dupliquées :\", duplicated_columns)\n",
    "\n",
    "    return {\"total_duplicated_rows\": total_duplicated_rows, \"duplicated_columns\": duplicated_columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c1a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptage du taux de données manquantes dans le DataFrame\n",
    "\n",
    "def pie_nan(dataframe):\n",
    "    \"\"\"Affiche un graphique en camembert montrant le taux de données manquantes dans le DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pandas.DataFrame): Le DataFrame à analyser.\n",
    "    \"\"\"\n",
    "    # Calcul du nombre de lignes et de colonnes\n",
    "    lignes = dataframe.shape[0]\n",
    "    colonnes = dataframe.shape[1]\n",
    "\n",
    "    # Nombre de données non manquantes\n",
    "    nb_data = dataframe.count().sum()\n",
    "\n",
    "    # Nombre total de données dans le jeu de données (colonnes * lignes)\n",
    "    nb_total = colonnes * lignes\n",
    "\n",
    "    # Taux de remplissage du jeu de données\n",
    "    rate_data_ok = nb_data / nb_total\n",
    "    print(f\"Le jeu de données est rempli à {rate_data_ok:.2%}\")\n",
    "    print(f\"Il a {1 - rate_data_ok:.2%} de données manquantes\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Création du pie chart\n",
    "    rates = [rate_data_ok, 1 - rate_data_ok]\n",
    "    labels = [\"Données\", \"NAN\"]\n",
    "    explode = (0, 0.1)\n",
    "    colors = ['gold', 'pink']\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    plt.pie(rates, explode=explode, labels=labels, colors=colors,\n",
    "            autopct='%.2f%%', shadow=True, textprops={'fontsize': 26})\n",
    "\n",
    "    # Titre du graphique\n",
    "    ttl = plt.title(\"Taux des valeurs manquantes dans la base de données\", fontsize=30)\n",
    "    ttl.set_position([0.5, 0.50])\n",
    "\n",
    "    # Assurer que le graphique est bien circulaire\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95dcb7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichege du pourcentage de valeurs manquantes par colonne\n",
    "\n",
    "def show_all_missing(data_set):\n",
    "    \"\"\"Calcule et retourne le pourcentage de valeurs manquantes par colonne dans un DataFrame.\n",
    "    \"\"\"\n",
    "    return (data_set.isna().sum() / data_set.shape[0]).sort_values(ascending=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a9f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_commune_insee(dept_code, commune_code):\n",
    "    \"\"\"\n",
    "    Construit le code INSEE à partir du code département et code commune.\n",
    "    \"\"\"\n",
    "    # Gestion des départements alphanumériques (ex: '2A', '2B') ou numériques\n",
    "    dept_str = str(dept_code).zfill(2) if str(dept_code).isnumeric() else str(dept_code)\n",
    "    commune_str = str(commune_code).zfill(3)\n",
    "    return dept_str + commune_str\n",
    "\n",
    "# Création de nouvelle colonne 'commune_insee' à partir de dataset valeur foncière\n",
    "def create_commune_insee(df, dept_col, commune_col):\n",
    "    \n",
    "    # Application de la fonction à chaque ligne du DataFrame\n",
    "    df['commune_insee'] = df.apply(lambda row: format_commune_insee(row[dept_col], row[commune_col]), axis=1)\n",
    "\n",
    "    # Affichage des premières lignes pour validation\n",
    "    print(\"Exemple de colonne 'commune_insee' :\")\n",
    "    print(df[[dept_col, commune_col, 'commune_insee']].head())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0900fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de 0 devant les code postaux à 4 chiffres et suppression de .0 dans le dataset valeur foncière\n",
    "def process_code_postal(df):\n",
    "    \n",
    "    # Convertir la colonne en chaîne de caractères\n",
    "    df['Code postal'] = df['Code postal'].astype(str)\n",
    "\n",
    "    # Supprimer le .0 des codes postaux\n",
    "    df['Code postal'] = df['Code postal'].str.replace(r'\\.0$', '', regex=True)\n",
    "\n",
    "    # Ajouter un 0 devant les codes à 4 chiffres\n",
    "    df['Code postal'] = df['Code postal'].apply(lambda x: x.zfill(5) if len(x) == 4 else x)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b6be73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d’un 0 devant les codes postaux à 4 chiffres dans la variable code_postal du dataset region.\n",
    "def process_code_postal_region(df):\n",
    "    df['code_postal'] = df['code_postal'].astype(str).apply(lambda x: '0' + x if len(x) == 4 else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3268b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de type des variables\n",
    "def transform_data_types(df, col_types):\n",
    "    for col, dtype in col_types.items():\n",
    "        if dtype == 'float64':\n",
    "            # If the target type is float64, replace commas with dots before conversion\n",
    "            # Check if the column contains strings before applying str.replace\n",
    "            if df[col].dtype == 'object': # If column is of type object, potentially containing string values\n",
    "                df[col] = pd.to_numeric(df[col].str.replace(',', '.', regex=False), errors='coerce')\n",
    "            else:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce') # Attempt direct numeric conversion if not an object\n",
    "        else:\n",
    "            df[col] = df[col].astype(dtype)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeaa5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des colonnes annee_mutation, mois_mutation\n",
    "def dateFiltered(df):\n",
    "    df['Date mutation'] = pd.to_datetime(df['Date mutation'], format='%d/%m/%Y', errors='coerce') # Conversion de la colonne 'Date mutation' en type datetime \n",
    "    df[\"annee_mutation\"] = df['Date mutation'].dt.year\n",
    "    df[\"mois_mutation\"] = df['Date mutation'].dt.month\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d40c4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des valeurs abérantes\n",
    "def detect_outliers_iqr(df):\n",
    "    \"\"\"\n",
    "    Supprime les valeurs aberrantes des colonnes numériques du DataSet\n",
    "    en utilisant la méthode de l'écart interquartile (IQR).\n",
    "    \"\"\"\n",
    "    # Identifier automatiquement les colonnes numériques (float ou int)\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "    # Supprimer les outliers pour chaque colonne numérique\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "\n",
    "        df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a9b45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, method='median', fill_value=None):\n",
    "    \n",
    "    # Traitement spécifique pour la colonne 'Type de voie'\n",
    "    if 'Type de voie' in df.columns:\n",
    "        if not df['Type de voie'].dropna().empty:\n",
    "            df['Type de voie'] = df['Type de voie'].fillna(df['Type de voie'].mode()[0])\n",
    "    \n",
    "    if method == 'drop':\n",
    "        df = df.dropna()  # Supprime les lignes contenant des valeurs NaN\n",
    "\n",
    "    elif method == 'fill':\n",
    "        if fill_value is None:\n",
    "            raise ValueError(\"Un 'fill_value' doit être spécifié pour la méthode 'fill'.\")\n",
    "        df = df.fillna(fill_value)  # Remplit toutes les valeurs NaN avec la valeur spécifiée\n",
    "\n",
    "    elif method == 'median':\n",
    "        # Remplir les colonnes numériques avec leur médiane\n",
    "        for col in df.select_dtypes(include=['number']).columns:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "        # Remplir les colonnes non numériques avec la valeur la plus fréquente (mode)\n",
    "        for col in df.select_dtypes(exclude=['number']).columns:\n",
    "            if col != 'Type de voie' and not df[col].dropna().empty:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Méthode non valide fournie. Utilisez 'drop', 'fill' ou 'median'.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07fd39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des doublons dans le dataSet\n",
    "def remove_duplicates(df, subset=None):\n",
    "\n",
    "    return df.drop_duplicates(subset=subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62087f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des variables numériques\n",
    "def scale_numeric_features(df):\n",
    "    \n",
    "    # Variables numériques continues à scaler\n",
    "    numeric_columns = ['Surface reelle bati', 'Nombre pieces principales',\n",
    "                       'Surface terrain', 'prix_m2', 'annee_mutation', \n",
    "                       'mois_mutation']\n",
    "    \n",
    "    # Appliquer le StandardScaler uniquement à ces colonnes\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0857959",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Nettoyage des variables\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munidecode\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_column_names\u001b[39m(columns):\n\u001b[32m      6\u001b[39m     df = []\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "# Nettoyage des variables\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "def clean_column_names(columns):\n",
    "    df = []\n",
    "    for col in columns:\n",
    "        # Enlever les accents\n",
    "        col = unidecode.unidecode(col)\n",
    "        # Remplacer les apostrophes et caractères spéciaux par rien ou un underscore\n",
    "        col = re.sub(r\"[^\\w\\s]\", \"\", col)\n",
    "        # Remplacer les espaces ou tirets par underscore\n",
    "        col = re.sub(r\"[\\s\\-]+\", \"_\", col)\n",
    "        df.append(col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73f73540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de heatmap de matrice de corrélation avec un seuil d'affichage des étiquettes de corrélation.\n",
    "def plot_half_correlation_matrix(matrix_num, thres=0.1, size=(10, 8), cmap=\"coolwarm\"):\n",
    "    \"\"\"\n",
    "    Ce texte décrit une fonction qui affiche une heatmap représentant la moitié supérieure \n",
    "    d'une matrice de corrélation. Elle permet de visualiser uniquement les coefficients \n",
    "    de corrélation supérieurs à un certain seuil. Les paramètres incluent la matrice \n",
    "    de corrélation (matrix_num), un seuil d’affichage (thres), \n",
    "    la taille de la figure (size) et la palette de couleurs utilisée (cmap).\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=size)\n",
    "    mask = np.triu(np.ones_like(matrix_num, dtype=bool))\n",
    "\n",
    "    # Création de la heatmap avec le masque de la moitié supérieure\n",
    "    ax = sns.heatmap(\n",
    "        matrix_num,\n",
    "        mask=mask,\n",
    "        cmap=cmap,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    plt.title(\"Matrice de Corrélation\")\n",
    "\n",
    "    # Ajuster les étiquettes annotées pour n'afficher que celles supérieures au seuil\n",
    "    for t in ax.texts:\n",
    "        try:\n",
    "            # Conversion en float si possible\n",
    "            value = float(t.get_text())\n",
    "            # Afficher ou masquer en fonction du seuil\n",
    "            if value >= thres:\n",
    "                t.set_text(f\"{value:.2f}\")\n",
    "            else:\n",
    "                t.set_text(\"\")\n",
    "        except ValueError:\n",
    "            # Si la conversion échoue, masquer l'étiquette\n",
    "            t.set_text(\"\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77c9a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: dans la base df_region, ajoute 0 devant tout nombre ayant 4 caractères dans la variable code_postal\n",
    "def process_code_postal_region(df):\n",
    "    df['code_postal'] = df['code_postal'].astype(str).apply(lambda x: '0' + x if len(x) == 4 else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eb0142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    # 0. Nettoyage des noms de colonnes dès le début pour homogénéiser les traitements\n",
    "    df.columns = clean_column_names(df.columns)\n",
    "\n",
    "    # 1. Suppression des doublons\n",
    "    df = remove_duplicates(df).copy()\n",
    "\n",
    "    # 2. Gestion des valeurs extrêmes pour toutes les colonnes\n",
    "    df = detect_outliers_iqr(df).copy()\n",
    "\n",
    "    # 3. Nettoyage spécifique de la variable cible 'Valeur_fonciere'\n",
    "    if 'Valeur_fonciere' in df.columns:\n",
    "        # a. Traitement des valeurs aberrantes (IQR)\n",
    "        q1 = df['Valeur_fonciere'].quantile(0.25)\n",
    "        q3 = df['Valeur_fonciere'].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        df = df[(df['Valeur_fonciere'] >= lower_bound) & (df['Valeur_fonciere'] <= upper_bound)].copy()\n",
    "\n",
    "        # b. Remplacement des valeurs manquantes par la médiane\n",
    "        median_valeur = df['Valeur_fonciere'].median()\n",
    "        df['Valeur_fonciere'] = df['Valeur_fonciere'].fillna(median_valeur)\n",
    "\n",
    "    # 4. Gestion des valeurs manquantes (mode pour catégorielles, médiane pour numériques)\n",
    "    df = handle_missing_values(df, method='median').copy()\n",
    "\n",
    "    # 5. Remplacer les infinis par NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 6. Clipper les valeurs extrêmes pour éviter les instabilités numériques\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].clip(lower=-1e10, upper=1e10) \n",
    "\n",
    "    # 7. Conversion des booléens en numériques\n",
    "    df = convert_bool_to_numeric(df)\n",
    "\n",
    "    # 8. Séparation X / y\n",
    "    X = df.drop(['Valeur_fonciere'], axis=1)\n",
    "    y = df['Valeur_fonciere']\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98ece6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation graphique de valeurs foncières par département\n",
    "# Cette fonction trace une carte colorée des départements selon une variable géospatiale (par défaut, la valeur foncière), discrétisée en classes optimales.\n",
    "def afficher_carte_departements(df_dep, departements, var_a_afficher=\"Valeur fonciere\", nom_variable_commune=\"Code departement\", cmap=\"RdYlGn_r\", nb_classes=5):\n",
    "\n",
    "    # 1. Fusionner les bases de données sur le code département\n",
    "    base = pd.merge(departements, df_dep, left_on=\"code\", right_on=nom_variable_commune)\n",
    "\n",
    "    # 2. Convertir la base en GeoDataFrame si elle ne l'est pas déjà\n",
    "    if not isinstance(base, gpd.GeoDataFrame):\n",
    "        base = gpd.GeoDataFrame(base, geometry=\"geometry\")  # Assurez-vous que la colonne 'geometry' existe\n",
    "\n",
    "    # 3. Gérer les NaN dans la colonne à afficher\n",
    "    base = base.dropna(subset=[var_a_afficher])\n",
    "\n",
    "    # 4. Convertir la variable à afficher en millions d'euros\n",
    "    base[var_a_afficher] = base[var_a_afficher] / 1000  # Conversion en millions d'euros\n",
    "\n",
    "    # 5. Discrétiser la variable à afficher (par Quantiles ici)\n",
    "    base['classe_discrete'] = pd.qcut(base[var_a_afficher], nb_classes, labels=False)\n",
    "\n",
    "    # 6. Créer la carte\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # 7. Afficher la carte avec un coloriage basé sur la variable discrétisée\n",
    "    base.plot(column='classe_discrete',\n",
    "              ax=ax,\n",
    "              legend=False,  # Désactiver la légende des classes discrètes\n",
    "              cmap=cmap)\n",
    "\n",
    "    # 8. Définir les limites de la vue (si pertinent pour votre zone géographique)\n",
    "    ax.set_xlim(-5, 10)  # Vous pouvez ajuster selon vos données géographiques\n",
    "    ax.set_ylim(41, 51)\n",
    "\n",
    "    # 9. Supprimer les axes pour une visualisation propre\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # 10. Ajouter une légende avec les valeurs réelles en millions d'euros\n",
    "    min_val = base[var_a_afficher].min()\n",
    "    max_val = base[var_a_afficher].max()\n",
    "\n",
    "    # Créer une normalisation basée sur les valeurs réelles\n",
    "    norm = mcolors.Normalize(vmin=min_val, vmax=max_val)\n",
    "    cmap_obj = plt.get_cmap(cmap)\n",
    "\n",
    "    # Créer une barre de couleur basée sur les valeurs réelles\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap_obj, norm=norm)\n",
    "    sm.set_array(base[var_a_afficher])\n",
    "\n",
    "    # Ajouter la barre de couleur avec des montants en millions d'euros\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(f\"{var_a_afficher} (en millions d'euros)\")\n",
    "\n",
    "    # 11. Afficher la carte\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "917deed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche une carte des communes colorée en fonction d'une variable discrétisée.\n",
    "def afficher_carte_communes(df_commu, codes_postaux, var_a_afficher=\"Valeur fonciere\",\n",
    "                            nom_variable_commune=\"Code postal\", cmap=\"RdYlGn_r\",\n",
    "                            nb_classes=5, methode_discretisation='quantile'):\n",
    "\n",
    "    # Fusion des données\n",
    "    base = codes_postaux.merge(df_commu, left_on=\"codePostal\", right_on=nom_variable_commune, how=\"left\").dropna(subset=[var_a_afficher])\n",
    "\n",
    "    # Conversion en numérique et en millions d'euros\n",
    "    base[var_a_afficher] = pd.to_numeric(base[var_a_afficher], errors='coerce') / 1_000_000\n",
    "\n",
    "    # Choix de la méthode de discrétisation\n",
    "    method_dict = {\n",
    "        'quantile': mc.Quantiles,\n",
    "        'uniform': mc.EqualInterval,\n",
    "        'jenks': mc.NaturalBreaks\n",
    "    }\n",
    "    \n",
    "    method_class = method_dict.get(methode_discretisation, mc.Quantiles)  # Par défaut : quantile\n",
    "\n",
    "    try:\n",
    "        classifier = method_class(base[var_a_afficher], k=nb_classes)\n",
    "        base['classe_discrete'] = classifier.yb  # Indices des classes\n",
    "        bins = classifier.bins  # Bornes des classes\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur de discrétisation : {e}\")\n",
    "        return\n",
    "\n",
    "    # Création de la carte\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    base.plot(column='classe_discrete', cmap=cmap, linewidth=0.5, edgecolor='black', ax=ax, legend=True)\n",
    "\n",
    "    # Suppression des axes pour un affichage propre\n",
    "    ax.set_axis_off()\n",
    "    plt.title(f\"Carte des valeurs foncières ({var_a_afficher} en M€)\", fontsize=12)\n",
    "\n",
    "    # Affichage\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15c006cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "import numpy as np\n",
    "\n",
    "def target_encoding(df_train_cleaned, df_test_cleaned):\n",
    "    # --- Target Encoding (manuelle) ---\n",
    "    for column in ['code_postal', 'Code_commune', 'Code_departement']:\n",
    "        mean_value_per_category = df_train_cleaned.groupby(column)['Valeur_fonciere'].mean()\n",
    "        global_mean = df_train_cleaned['Valeur_fonciere'].mean()\n",
    "\n",
    "        df_train_cleaned[column + '_encoded'] = df_train_cleaned[column].map(mean_value_per_category)\n",
    "        df_test_cleaned[column + '_encoded'] = df_test_cleaned[column].map(mean_value_per_category).fillna(global_mean)\n",
    "\n",
    "        df_train_cleaned.drop(columns=[column], inplace=True)\n",
    "        df_test_cleaned.drop(columns=[column], inplace=True)\n",
    "\n",
    "    # --- Target Encoding (avec category_encoders) pour haute cardinalité ---\n",
    "    high_card_cols = ['Voie', 'Type_de_voie']\n",
    "    encoder = TargetEncoder(cols=high_card_cols)\n",
    "\n",
    "    # Fit uniquement sur le trainFonction_utils\n",
    "    encoder.fit(df_train_cleaned[high_card_cols], df_train_cleaned['Valeur_fonciere'])\n",
    "\n",
    "    # Transform train et test\n",
    "    df_train_encoded = encoder.transform(df_train_cleaned[high_card_cols])\n",
    "    df_test_encoded = encoder.transform(df_test_cleaned[high_card_cols])\n",
    "\n",
    "    # Renommer les colonnes encodées\n",
    "    df_train_encoded.columns = [col + '_te' for col in high_card_cols]\n",
    "    df_test_encoded.columns = [col + '_te' for col in high_card_cols]\n",
    "\n",
    "    # Ajouter au dataframe principal\n",
    "    df_train_cleaned = df_train_cleaned.drop(columns=high_card_cols)\n",
    "    df_test_cleaned = df_test_cleaned.drop(columns=high_card_cols)\n",
    "\n",
    "    df_train_cleaned = pd.concat([df_train_cleaned, df_train_encoded], axis=1)\n",
    "    df_test_cleaned = pd.concat([df_test_cleaned, df_test_encoded], axis=1)\n",
    "\n",
    "    # --- One-Hot Encoding ---\n",
    "    df_train_cleaned = pd.get_dummies(df_train_cleaned, columns=['Nature_mutation', 'Type_local'], drop_first=False)\n",
    "    df_test_cleaned = pd.get_dummies(df_test_cleaned, columns=['Nature_mutation', 'Type_local'], drop_first=False)\n",
    "\n",
    "    # Harmoniser les colonnes entre les deux datasets\n",
    "    df_train_cleaned, df_test_cleaned = df_train_cleaned.align(df_test_cleaned, join='left', axis=1, fill_value=0)\n",
    "\n",
    "    return df_train_cleaned, df_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c6e2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir_colonnes_booleennes_en_entiers\n",
    "def convert_bool_to_numeric(df):\n",
    "    for col in df.select_dtypes(include='bool').columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2e467-84d0-4b08-9a0c-889f830aa1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0923c-856e-47df-98b8-6e47998f06dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7af6a-d082-4cda-b5f2-f4cd610c5a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (RAG)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
